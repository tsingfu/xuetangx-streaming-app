<configuation>

    <apps>
        <app id="1" interfaceId="1">
            <properties>
                <property>
                    <name>batchDuration.seconds</name>
                    <value>15</value>
                    <description></description>
                </property>
                <property>
                    <name>timeInterval.minutes.list</name>
                    <value>1</value>
                    <description>
                        示例: `1,5,10`
                        表示：需要增强的统计指标的时间维度1分钟，5分钟，10分钟
                    </description>
                </property>

                <property>
                    <name>kafka.simple.consumer.api.used</name>
                    <value>true</value>
                    <description></description>
                </property>

                <property>
                    <name>kafka.consumer.group.id</name>
                    <value></value>
                    <description>当 kafka.simple.consumer.api.used = false 设置</description>
                </property>

                <property>
                    <name>spark.streaming.kafka.maxRatePerPartition</name>
                    <description>当 kafka.simple.consumer.api.used = true 设置</description>
                </property>

                <property>
                    <name>spark.streaming.kafka.maxRetries</name>
                    <description>当 kafka.simple.consumer.api.used = true 设置</description>
                </property>


                <!--调优参数-->
                <property>
                    <name>spark.streaming.blockInterval</name>
                    <value>200</value>
                    <description>
                        Interval at which data received by Spark Streaming receivers is chunked into
                        blocks of data before storing them in Spark. Minimum recommended - 50 ms.
                    </description>
                </property>

                <property>
                    <name>spark.streaming.receiver.maxRate</name>
                    <value></value>
                    <description>Maximum rate (number of records per second) at which each receiver will receive data.
                        Effectively, each stream will consume at most this number of records per second.
                        Setting this configuration to 0 or a negative number will put no limit on the rate.
                        See the deployment guide in the Spark Streaming programing guide for mode details.
                    </description>
                </property>
            </properties>
        </app>
    </apps>

    <dataSources>
        <source id="1" type="kafka" name="kafka_cluster1">
            <properties>
                <property>
                    <name>zookeeper.connect</name>
                    <!--<value>hadoop1:9092,hadoop2:9092,hadoop3:9092</value>-->
                    <!--<value>192.168.9.243:2181,192.168.9.240:2181,192.168.9.190:2181</value>-->
                    <!--<value>192.168.9.243:2181</value>-->
                    <value>localhost:2181</value>
                    <description></description>
                </property>
                <property>
                    <name>metadata.broker.list</name>
                    <!--<value>hadoop1:9092,hadoop2:9092,hadoop3:9092</value>-->
                    <!--<value>192.168.9.243:9092,192.168.9.240:9092,192.168.9.190:9092,192.168.9.164:9092</value>-->
                    <!--org.apache.spark.SparkException: Couldn't find leader offsets for Set([topic1-platformlog,1], [topic2-vpclog,2], [topic1-platformlog,0], [topic2-vpclog,0], [topic2-vpclog,1], [topic1-platformlog,2])-->
                    <value>localhost:9092</value>
                    <description></description>
                </property>
            </properties>
        </source>

        <source id="2" type="jdbc" name="mysqldb1">
            <properties>
                <property>
                    <name>driver</name>
                    <value>com.mysql.db.db.jdbc.Driver</value>
                    <description></description>
                </property>
                <property>
                    <name>url</name>
                    <value>db.jdbc:mysql://mysql1:3306/xuetangx1</value>
                    <description></description>
                </property>
                <property>
                    <name>user</name>
                    <value>root</value>
                    <description></description>
                </property>
                <property>
                    <name>password</name>
                    <value>mysql123</value>
                    <description></description>
                </property>
            </properties>
        </source>

        <source id="3" type="mongo" name="mongodb1">
            <properties>
                <property>
                    <name>url</name>
                    <value>mongodb://192.168.9.164:27017</value>
                    <description></description>
                </property>
            </properties>
        </source>
    </dataSources>

    <dataInterfaces phase="1">
        <interface id="1" type="input" sourceId="1" name="kafka1">
            <properties>
                <property>
                    <name>topics</name>
                    <value>topic1-platformlog,topic2-vpclog</value>
                    <description></description>
                </property>
                <property>
                    <name>type</name>
                    <value>json</value>
                    <description></description>
                </property>
            </properties>
        </interface>

    </dataInterfaces>

    <!-- -->
    <externalCaches>
        <cache id="1" sourceId="2">
            <properties>
                <property>
                    <name>tableName</name>
                    <value>device_info</value>
                    <description></description>
                </property>
                <property>
                    <name>keyColumns</name>
                    <value>uuid</value>
                    <description></description>
                </property>
            </properties>
        </cache>
    </externalCaches>

    <prepares id="1" interfaceId="1" phase="2">
        <!--
                <step id="2" type="filter" method="spark-sql" enabled="true">
                    <properties>
                        <property>
                            <name>selectExprClause</name>
                            <value></value>
                            <description>
                                设置需要用到的字段名，流程：读入数据后，转换为spark-SQL的DataFrame，然后进行 selectExp，仅保留需要的数据
                                空时，默认取*
                            </description>
                        </property>
                        <property>
                            <name>whereClause</name>
                            <value></value>
                            <description></description>
                        </property>
                    </properties>
                </step>
                <step id="3" type="filter" method="plugin" enabled="true">
                    <properties>
                        <property>
                            <name>class</name>
                            <value>com.xuetangx.streaming.process.StreamingProcessorDemo1</value>
                            <description></description>
                        </property>
                    </properties>
                </step>
                <step id="4" type="enhance" method="plugin" enabled="false">
                    <properties>
                        <property>
                            <name>class</name>
                            <value>com.xuetangx.streaming.process.StreamingProcessorDemo1</value>
                            <description></description>
                        </property>
                        <property>
                            <name>cacheManager</name>
                            <value>JdbcCacheManager</value>
                            <description></description>
                        </property>
                    </properties>
                </step>
                <step id="5" type="enhance" method="spark-sql" enabled="false">
                    <properties>
                        <property>
                            <name>selectExprClause</name>
                            <value></value>
                            <description></description>
                        </property>
                    </properties>
                </step>
        -->
    </prepares>

    <prepares preparesId="2" interfaceId="2" phase="2"></prepares>

    <!--1期不支持多个流数据上不同指标的合并-->
    <computeStatistics id="1" interfaceId="1" phase="3">
        <computeStatistic id="1" enabled="true">
            <prepares>
                <step id="1" type="filter" method="spark-sql" enabled="true">
                    <properties>
                        <property>
                            <name>selectExprClause</name>
                            <value></value>
                            <description>设置需要用到的字段名，流程：读入数据后，转换为spark-SQL的DataFrame，然后进行 selectExp，仅保留需要的数据</description>
                        </property>
                        <property>
                            <name>whereClause</name>
                            <value></value>
                            <description></description>
                        </property>
                    </properties>
                </step>

                <step id="2" type="enhance" method="plugin" enabled="true">
                    <properties>
                        <property>
                            <name>class</name>
                            <value>com.xuetangx.streaming.prepares.StreamingProcessorDemo1</value>
                            <description></description>
                        </property>
                        <property>
                            <name>cacheManager</name>
                            <value>JdbcCacheManager</value>
                            <description></description>
                        </property>
                    </properties>
                </step>

                <step id="3" type="batchDeduplicate" method="spark-sql" enabled="true">
                    <properties>
                        <property>
                            <name>unique.key</name>
                            <value>uid</value>
                            <description>批次间排重</description>
                        </property>
                    </properties>
                </step>

            </prepares>

            <computes>
                <step id="4" type="compute" method="spark-sql" enabled="true">
                    <!-- 计算后的结果形式 -->
                    <properties>
                        <property>
                            <name>statisticKeyMap</name>
                            <!--<value>T1:time_min_1,L1:host,L2:platform,L3.1:origin_referer,L3.2:spam</value>-->
                            <value>L1:uid,L2:platform,L3.1:origin_referer</value>
                            <description></description>
                        </property>

                        <property>
                            <name>targetKeysList</name>
                            <!--<value>L1:T1,L2:T1,L1:L2:T1</value>-->
                            <value>L1,L2,L1:L2</value>
                            <description>各统计指标维度的keys列表</description>
                        </property>

                        <property>
                            <name>uk.method.label.list</name>
                            <value>uid:countDistinct:users</value>
                            <!--<value>uid:count:users</value>-->
                            <description>
                                uk: count/countDistinct的字段名
                                method: count/countDistinct/sum 聚合操作的函数名
                                label: 统计标签类型
                            </description>
                        </property>

                    </properties>
                </step>
            </computes>

            <!-- TODO: 输出 -->
        </computeStatistic>

    </computeStatistics>

    <computeStatistics id="2" interfaceId="2" phase="3"></computeStatistics>

</configuation>
